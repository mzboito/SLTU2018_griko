08/02 21:36:00 label: griko best setup 1
08/02 21:36:00 description:
  temp = 1
08/02 21:36:00 /home/getalp/zanonbom/seq2seq/translate/__main__.py ../SLTU_griko/models/grapheme/random_split/rand5/config_grapheme.yaml --train -v
08/02 21:36:00 commit hash d7914bbb0f1dc22438de0e15b82ebec43e0614ff
08/02 21:36:00 tensorflow version: 1.8.0
08/02 21:36:00 program arguments
08/02 21:36:00   aggregation_method   'concat'
08/02 21:36:00   align_encoder_id     0
08/02 21:36:00   allow_growth         True
08/02 21:36:00   attention_type       'global'
08/02 21:36:00   attn_filter_length   0
08/02 21:36:00   attn_filters         0
08/02 21:36:00   attn_prev_word       False
08/02 21:36:00   attn_size            12
08/02 21:36:00   attn_temperature     1
08/02 21:36:00   attn_window_size     0
08/02 21:36:00   average              False
08/02 21:36:00   baseline_activation  None
08/02 21:36:00   baseline_learning_rate 0.001
08/02 21:36:00   baseline_optimizer   'adam'
08/02 21:36:00   baseline_steps       0
08/02 21:36:00   batch_mode           'standard'
08/02 21:36:00   batch_size           32
08/02 21:36:00   beam_size            1
08/02 21:36:00   bidir                True
08/02 21:36:00   bidir_projection     False
08/02 21:36:00   binary               False
08/02 21:36:00   cell_size            12
08/02 21:36:00   cell_type            'LSTM'
08/02 21:36:00   character_level      False
08/02 21:36:00   checkpoints          []
08/02 21:36:00   conditional_rnn      False
08/02 21:36:00   config               '../SLTU_griko/models/grapheme/random_split/rand5/config_grapheme.yaml'
08/02 21:36:00   convolutions         None
08/02 21:36:00   data_dir             '../SLTU_griko/models/grapheme/random_split/files/'
08/02 21:36:00   debug                False
08/02 21:36:00   decay_after_n_epoch  0
08/02 21:36:00   decay_every_n_epoch  None
08/02 21:36:00   decay_if_no_progress None
08/02 21:36:00   decoders             [{'name': 'gr'}]
08/02 21:36:00   description          'temp = 1'
08/02 21:36:00   dev_prefix           ['dev.rand5', 'train.rand5']
08/02 21:36:00   embedding_dropout    0.0
08/02 21:36:00   embedding_initializer None
08/02 21:36:00   embedding_size       12
08/02 21:36:00   embedding_weight_scale None
08/02 21:36:00   encoders             [{'name': 'it'}]
08/02 21:36:00   ensemble             False
08/02 21:36:00   eval_burn_in         0
08/02 21:36:00   feed_previous        0.0
08/02 21:36:00   final_state          'last'
08/02 21:36:00   freeze_variables     []
08/02 21:36:00   generate_first       True
08/02 21:36:00   gpu_id               0
08/02 21:36:00   highway_layers       0
08/02 21:36:00   initial_state_dropout 0.5
08/02 21:36:00   initializer          None
08/02 21:36:00   input_layer_dropout  0.0
08/02 21:36:00   input_layers         None
08/02 21:36:00   keep_best            4
08/02 21:36:00   keep_every_n_hours   0
08/02 21:36:00   label                'griko best setup 1'
08/02 21:36:00   layer_norm           False
08/02 21:36:00   layers               1
08/02 21:36:00   learning_rate        0.001
08/02 21:36:00   learning_rate_decay_factor 1.0
08/02 21:36:00   len_normalization    1.0
08/02 21:36:00   log_file             'log_griko.txt'
08/02 21:36:00   loss_function        'xent'
08/02 21:36:00   max_dev_size         0
08/02 21:36:00   max_epochs           0
08/02 21:36:00   max_gradient_norm    5.0
08/02 21:36:00   max_len              128
08/02 21:36:00   max_steps            150000
08/02 21:36:00   max_test_size        0
08/02 21:36:00   max_to_keep          1
08/02 21:36:00   max_train_size       0
08/02 21:36:00   maxout_stride        None
08/02 21:36:00   mem_fraction         1.0
08/02 21:36:00   min_learning_rate    1e-06
08/02 21:36:00   model_dir            '../SLTU_griko/models/grapheme/random_split/rand5/model/'
08/02 21:36:00   moving_average       None
08/02 21:36:00   no_gpu               False
08/02 21:36:00   optimizer            'adam'
08/02 21:36:00   orthogonal_init      False
08/02 21:36:00   output               None
08/02 21:36:00   output_dropout       0.0
08/02 21:36:00   parallel_iterations  16
08/02 21:36:00   pervasive_dropout    False
08/02 21:36:00   pooling_avg          True
08/02 21:36:00   post_process_script  None
08/02 21:36:00   pred_deep_layer      False
08/02 21:36:00   pred_edits           False
08/02 21:36:00   pred_embed_proj      False
08/02 21:36:00   pred_maxout_layer    True
08/02 21:36:00   purge                False
08/02 21:36:00   raw_output           False
08/02 21:36:00   read_ahead           10
08/02 21:36:00   reconstruction_attn_weight 0.05
08/02 21:36:00   reconstruction_decoders False
08/02 21:36:00   reconstruction_weight 1.0
08/02 21:36:00   reinforce_after_n_epoch None
08/02 21:36:00   remove_unk           False
08/02 21:36:00   reverse              False
08/02 21:36:00   reverse_input        False
08/02 21:36:00   reward_function      'sentence_bleu'
08/02 21:36:00   rnn_feed_attn        True
08/02 21:36:00   rnn_input_dropout    0.5
08/02 21:36:00   rnn_output_dropout   0.0
08/02 21:36:00   rnn_state_dropout    0.0
08/02 21:36:00   save                 False
08/02 21:36:00   score_function       'corpus_bleu'
08/02 21:36:00   score_functions      ['bleu', 'loss']
08/02 21:36:00   script_dir           'scripts'
08/02 21:36:00   sgd_after_n_epoch    None
08/02 21:36:00   sgd_learning_rate    1.0
08/02 21:36:00   shuffle              True
08/02 21:36:00   softmax_temperature  1.0
08/02 21:36:00   steps_per_checkpoint 10000
08/02 21:36:00   steps_per_eval       10000
08/02 21:36:00   swap_memory          True
08/02 21:36:00   tie_embeddings       False
08/02 21:36:00   time_pooling         None
08/02 21:36:00   train                True
08/02 21:36:00   train_initial_states True
08/02 21:36:00   train_prefix         'train.rand5'
08/02 21:36:00   truncate_lines       True
08/02 21:36:00   update_first         False
08/02 21:36:00   use_baseline         False
08/02 21:36:00   use_dropout          True
08/02 21:36:00   use_lstm             True
08/02 21:36:00   use_lstm_full_state  False
08/02 21:36:00   use_previous_word    True
08/02 21:36:00   verbose              True
08/02 21:36:00   vocab_prefix         'vocab.rand5'
08/02 21:36:00   weight_scale         0.1
08/02 21:36:00   word_dropout         0.0
08/02 21:36:00 python random seed: 6866962620554171778
08/02 21:36:00 tf random seed:     1464233828571696783
08/02 21:36:00 creating model
08/02 21:36:00 using device: /gpu:0
08/02 21:36:00 copying vocab to ../SLTU_griko/models/grapheme/random_split/rand5/model/data/vocab.it
08/02 21:36:00 copying vocab to ../SLTU_griko/models/grapheme/random_split/rand5/model/data/vocab.gr
08/02 21:36:00 reading vocabularies
08/02 21:36:00 creating model
08/02 21:36:08 model parameters (27)
08/02 21:36:08   baseline_step:0 ()
08/02 21:36:08   decoder_gr/attention_it/U_a/kernel:0 (24, 12)
08/02 21:36:08   decoder_gr/attention_it/W_a/bias:0 (12,)
08/02 21:36:08   decoder_gr/attention_it/W_a/kernel:0 (12, 12)
08/02 21:36:08   decoder_gr/attention_it/v_a:0 (12,)
08/02 21:36:08   decoder_gr/basic_lstm_cell/bias:0 (48,)
08/02 21:36:08   decoder_gr/basic_lstm_cell/kernel:0 (48, 48)
08/02 21:36:08   decoder_gr/it/initial_state_projection/bias:0 (24,)
08/02 21:36:08   decoder_gr/it/initial_state_projection/kernel:0 (12, 24)
08/02 21:36:08   decoder_gr/maxout/bias:0 (12,)
08/02 21:36:08   decoder_gr/maxout/kernel:0 (48, 12)
08/02 21:36:08   decoder_gr/softmax1/bias:0 (44,)
08/02 21:36:08   decoder_gr/softmax1/kernel:0 (6, 44)
08/02 21:36:08   embedding_gr:0 (44, 12)
08/02 21:36:08   embedding_it:0 (441, 12)
08/02 21:36:08   encoder_it/initial_state_bw:0 (24,)
08/02 21:36:08   encoder_it/initial_state_fw:0 (24,)
08/02 21:36:08   encoder_it/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/basic_lstm_cell/bias:0 (48,)
08/02 21:36:08   encoder_it/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/basic_lstm_cell/kernel:0 (24, 48)
08/02 21:36:08   encoder_it/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/bias:0 (48,)
08/02 21:36:08   encoder_it/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/kernel:0 (24, 48)
08/02 21:36:08   global_step:0 ()
08/02 21:36:08   initial_state_keep_prob:0 ()
08/02 21:36:08   initial_state_keep_prob_1:0 ()
08/02 21:36:08   learning_rate:0 ()
08/02 21:36:08   rnn_input_keep_prob:0 ()
08/02 21:36:08   rnn_input_keep_prob_1:0 ()
08/02 21:36:08 number of parameters: 0.01M
08/02 21:36:09 global step: 0
08/02 21:36:09 baseline step: 0
08/02 21:36:09 reading training data
08/02 21:36:09 total line count: 297
08/02 21:36:09 files: ../SLTU_griko/models/grapheme/random_split/files/train.rand5.it ../SLTU_griko/models/grapheme/random_split/files/train.rand5.gr
08/02 21:36:09 lines reads: 297
08/02 21:36:09 reading development data
08/02 21:36:09 files: ../SLTU_griko/models/grapheme/random_split/files/dev.rand5.it ../SLTU_griko/models/grapheme/random_split/files/dev.rand5.gr
08/02 21:36:09 lines reads: 33
08/02 21:36:09 files: ../SLTU_griko/models/grapheme/random_split/files/train.rand5.it ../SLTU_griko/models/grapheme/random_split/files/train.rand5.gr
08/02 21:36:09 lines reads: 297
08/02 21:36:09 starting training
08/02 21:53:55 step 10000 epoch 1078 learning rate 0.001 step-time 0.105 loss 80.268
08/02 21:53:55 starting evaluation
08/02 21:54:10 dev.rand5 bleu=9.76 loss=82.44 penalty=1.000 ratio=1.190
08/02 21:54:13 train.rand5 bleu=12.74 loss=56.59 penalty=1.000 ratio=1.216
08/02 21:54:13 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/02 21:54:13 finished saving model
08/02 21:54:13 new best model
08/02 22:11:52 step 20000 epoch 2155 learning rate 0.001 step-time 0.105 loss 67.789
08/02 22:11:52 starting evaluation
08/02 22:11:53 dev.rand5 bleu=14.52 loss=80.49 penalty=1.000 ratio=1.058
08/02 22:11:56 train.rand5 bleu=19.99 loss=49.32 penalty=1.000 ratio=1.054
08/02 22:11:56 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/02 22:11:56 finished saving model
08/02 22:11:56 new best model
08/02 22:29:36 step 30000 epoch 3233 learning rate 0.001 step-time 0.105 loss 63.840
08/02 22:29:36 starting evaluation
08/02 22:29:37 dev.rand5 bleu=13.71 loss=82.54 penalty=1.000 ratio=1.069
08/02 22:29:40 train.rand5 bleu=19.99 loss=46.10 penalty=1.000 ratio=1.077
08/02 22:29:40 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/02 22:29:40 finished saving model
08/02 22:47:34 step 40000 epoch 4310 learning rate 0.001 step-time 0.106 loss 61.672
08/02 22:47:34 starting evaluation
08/02 22:47:35 dev.rand5 bleu=13.96 loss=83.05 penalty=1.000 ratio=1.000
08/02 22:47:38 train.rand5 bleu=22.59 loss=44.42 penalty=1.000 ratio=1.035
08/02 22:47:38 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/02 22:47:38 finished saving model
08/02 23:05:46 step 50000 epoch 5388 learning rate 0.001 step-time 0.107 loss 60.395
08/02 23:05:46 starting evaluation
08/02 23:05:46 dev.rand5 bleu=16.51 loss=83.15 penalty=1.000 ratio=1.047
08/02 23:05:50 train.rand5 bleu=22.24 loss=43.73 penalty=1.000 ratio=1.071
08/02 23:05:50 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/02 23:05:50 finished saving model
08/02 23:05:50 new best model
08/02 23:23:56 step 60000 epoch 6465 learning rate 0.001 step-time 0.107 loss 59.455
08/02 23:23:56 starting evaluation
08/02 23:23:57 dev.rand5 bleu=14.74 loss=82.81 penalty=1.000 ratio=1.012
08/02 23:24:00 train.rand5 bleu=23.91 loss=42.70 penalty=1.000 ratio=1.041
08/02 23:24:00 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/02 23:24:00 finished saving model
08/02 23:42:08 step 70000 epoch 7543 learning rate 0.001 step-time 0.107 loss 58.643
08/02 23:42:08 starting evaluation
08/02 23:42:08 dev.rand5 bleu=15.79 loss=84.16 penalty=0.993 ratio=0.993
08/02 23:42:12 train.rand5 bleu=24.47 loss=42.08 penalty=1.000 ratio=1.018
08/02 23:42:12 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/02 23:42:12 finished saving model
08/03 00:00:18 step 80000 epoch 8620 learning rate 0.001 step-time 0.107 loss 58.049
08/03 00:00:18 starting evaluation
08/03 00:00:19 dev.rand5 bleu=14.61 loss=84.95 penalty=1.000 ratio=1.059
08/03 00:00:22 train.rand5 bleu=25.57 loss=42.06 penalty=1.000 ratio=1.023
08/03 00:00:22 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 00:00:22 finished saving model
08/03 00:18:30 step 90000 epoch 9697 learning rate 0.001 step-time 0.107 loss 57.536
08/03 00:18:30 starting evaluation
08/03 00:18:31 dev.rand5 bleu=14.66 loss=85.91 penalty=0.989 ratio=0.989
08/03 00:18:34 train.rand5 bleu=25.21 loss=41.56 penalty=0.956 ratio=0.957
08/03 00:18:34 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 00:18:34 finished saving model
08/03 00:36:42 step 100000 epoch 10775 learning rate 0.001 step-time 0.107 loss 57.082
08/03 00:36:42 starting evaluation
08/03 00:36:43 dev.rand5 bleu=15.78 loss=85.41 penalty=1.000 ratio=1.043
08/03 00:36:46 train.rand5 bleu=26.81 loss=41.05 penalty=0.970 ratio=0.971
08/03 00:36:46 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 00:36:46 finished saving model
08/03 00:54:51 step 110000 epoch 11852 learning rate 0.001 step-time 0.107 loss 56.700
08/03 00:54:51 starting evaluation
08/03 00:54:51 dev.rand5 bleu=12.47 loss=85.90 penalty=1.000 ratio=1.049
08/03 00:54:55 train.rand5 bleu=24.48 loss=40.82 penalty=1.000 ratio=1.027
08/03 00:54:55 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 00:54:55 finished saving model
08/03 01:13:03 step 120000 epoch 12930 learning rate 0.001 step-time 0.107 loss 56.319
08/03 01:13:03 starting evaluation
08/03 01:13:03 dev.rand5 bleu=14.09 loss=87.15 penalty=1.000 ratio=1.092
08/03 01:13:06 train.rand5 bleu=24.61 loss=41.24 penalty=1.000 ratio=1.078
08/03 01:13:06 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 01:13:06 finished saving model
08/03 01:31:14 step 130000 epoch 14007 learning rate 0.001 step-time 0.107 loss 56.022
08/03 01:31:14 starting evaluation
08/03 01:31:14 dev.rand5 bleu=15.08 loss=86.97 penalty=1.000 ratio=1.057
08/03 01:31:17 train.rand5 bleu=24.66 loss=40.65 penalty=1.000 ratio=1.024
08/03 01:31:17 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 01:31:17 finished saving model
08/03 01:49:24 step 140000 epoch 15085 learning rate 0.001 step-time 0.107 loss 55.781
08/03 01:49:24 starting evaluation
08/03 01:49:24 dev.rand5 bleu=12.49 loss=86.90 penalty=1.000 ratio=1.029
08/03 01:49:27 train.rand5 bleu=24.79 loss=40.57 penalty=1.000 ratio=1.040
08/03 01:49:27 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 01:49:27 finished saving model
08/03 02:07:33 step 150000 epoch 16162 learning rate 0.001 step-time 0.107 loss 55.581
08/03 02:07:33 starting evaluation
08/03 02:07:34 dev.rand5 bleu=12.88 loss=86.94 penalty=1.000 ratio=1.045
08/03 02:07:36 train.rand5 bleu=25.46 loss=40.62 penalty=1.000 ratio=1.042
08/03 02:07:36 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 02:07:36 finished saving model
08/03 02:07:37 finished training
08/03 02:07:37 exiting...
08/03 02:07:37 saving model to ../SLTU_griko/models/grapheme/random_split/rand5/model/checkpoints
08/03 02:07:37 finished saving model
