06/08 10:48:08 label: griko best setup 1
06/08 10:48:08 description:
  temp = 1
06/08 10:48:08 /home/mzboito/Documents/seq2seq/translate/__main__.py ../SLTU_experiments/grapheme/exp2/config_grapheme.yaml --train -v
06/08 10:48:08 commit hash fa2755653a5ec0abf6bb38ed222ffa0cca4115c8
06/08 10:48:08 tensorflow version: 1.8.0
06/08 10:48:08 program arguments
06/08 10:48:08   aggregation_method   'concat'
06/08 10:48:08   align_encoder_id     0
06/08 10:48:08   allow_growth         True
06/08 10:48:08   attention_type       'global'
06/08 10:48:08   attn_filter_length   0
06/08 10:48:08   attn_filters         0
06/08 10:48:08   attn_prev_word       False
06/08 10:48:08   attn_size            12
06/08 10:48:08   attn_temperature     1
06/08 10:48:08   attn_window_size     0
06/08 10:48:08   average              False
06/08 10:48:08   baseline_activation  None
06/08 10:48:08   baseline_learning_rate 0.001
06/08 10:48:08   baseline_optimizer   'adam'
06/08 10:48:08   baseline_steps       0
06/08 10:48:08   batch_mode           'standard'
06/08 10:48:08   batch_size           32
06/08 10:48:08   beam_size            1
06/08 10:48:08   bidir                True
06/08 10:48:08   bidir_projection     False
06/08 10:48:08   binary               False
06/08 10:48:08   cell_size            12
06/08 10:48:08   cell_type            'LSTM'
06/08 10:48:08   character_level      False
06/08 10:48:08   checkpoints          []
06/08 10:48:08   conditional_rnn      False
06/08 10:48:08   config               '../SLTU_experiments/grapheme/exp2/config_grapheme.yaml'
06/08 10:48:08   convolutions         None
06/08 10:48:08   data_dir             '../SLTU_experiments/grapheme/files/'
06/08 10:48:08   debug                False
06/08 10:48:08   decay_after_n_epoch  0
06/08 10:48:08   decay_every_n_epoch  None
06/08 10:48:08   decay_if_no_progress None
06/08 10:48:08   decoders             [{'name': 'gr'}]
06/08 10:48:08   description          'temp = 1'
06/08 10:48:08   dev_prefix           ['dev', 'train']
06/08 10:48:08   embedding_dropout    0.0
06/08 10:48:08   embedding_initializer None
06/08 10:48:08   embedding_size       12
06/08 10:48:08   embedding_weight_scale None
06/08 10:48:08   encoders             [{'name': 'it'}]
06/08 10:48:08   ensemble             False
06/08 10:48:08   eval_burn_in         0
06/08 10:48:08   feed_previous        0.0
06/08 10:48:08   final_state          'last'
06/08 10:48:08   freeze_variables     []
06/08 10:48:08   generate_first       True
06/08 10:48:08   gpu_id               0
06/08 10:48:08   highway_layers       0
06/08 10:48:08   initial_state_dropout 0.5
06/08 10:48:08   initializer          None
06/08 10:48:08   input_layer_dropout  0.0
06/08 10:48:08   input_layers         None
06/08 10:48:08   keep_best            4
06/08 10:48:08   keep_every_n_hours   0
06/08 10:48:08   label                'griko best setup 1'
06/08 10:48:08   layer_norm           False
06/08 10:48:08   layers               1
06/08 10:48:08   learning_rate        0.001
06/08 10:48:08   learning_rate_decay_factor 1.0
06/08 10:48:08   len_normalization    1.0
06/08 10:48:08   log_file             'log_griko.txt'
06/08 10:48:08   loss_function        'xent'
06/08 10:48:08   max_dev_size         0
06/08 10:48:08   max_epochs           0
06/08 10:48:08   max_gradient_norm    5.0
06/08 10:48:08   max_len              128
06/08 10:48:08   max_steps            150000
06/08 10:48:08   max_test_size        0
06/08 10:48:08   max_to_keep          1
06/08 10:48:08   max_train_size       0
06/08 10:48:08   maxout_stride        None
06/08 10:48:08   mem_fraction         1.0
06/08 10:48:08   min_learning_rate    1e-06
06/08 10:48:08   model_dir            '../SLTU_experiments/grapheme/exp2/model/'
06/08 10:48:08   moving_average       None
06/08 10:48:08   no_gpu               False
06/08 10:48:08   optimizer            'adam'
06/08 10:48:08   orthogonal_init      False
06/08 10:48:08   output               None
06/08 10:48:08   output_dropout       0.0
06/08 10:48:08   parallel_iterations  16
06/08 10:48:08   pervasive_dropout    False
06/08 10:48:08   pooling_avg          True
06/08 10:48:08   post_process_script  None
06/08 10:48:08   pred_deep_layer      False
06/08 10:48:08   pred_edits           False
06/08 10:48:08   pred_embed_proj      False
06/08 10:48:08   pred_maxout_layer    True
06/08 10:48:08   purge                False
06/08 10:48:08   raw_output           False
06/08 10:48:08   read_ahead           10
06/08 10:48:08   reconstruction_attn_weight 0.05
06/08 10:48:08   reconstruction_decoders False
06/08 10:48:08   reconstruction_weight 1.0
06/08 10:48:08   reinforce_after_n_epoch None
06/08 10:48:08   remove_unk           False
06/08 10:48:08   reverse              False
06/08 10:48:08   reverse_input        False
06/08 10:48:08   reward_function      'sentence_bleu'
06/08 10:48:08   rnn_feed_attn        True
06/08 10:48:08   rnn_input_dropout    0.5
06/08 10:48:08   rnn_output_dropout   0.0
06/08 10:48:08   rnn_state_dropout    0.0
06/08 10:48:08   save                 False
06/08 10:48:08   score_function       'corpus_bleu'
06/08 10:48:08   score_functions      ['bleu', 'loss']
06/08 10:48:08   script_dir           'scripts'
06/08 10:48:08   sgd_after_n_epoch    None
06/08 10:48:08   sgd_learning_rate    1.0
06/08 10:48:08   shuffle              True
06/08 10:48:08   softmax_temperature  1.0
06/08 10:48:08   steps_per_checkpoint 10000
06/08 10:48:08   steps_per_eval       10000
06/08 10:48:08   swap_memory          True
06/08 10:48:08   tie_embeddings       False
06/08 10:48:08   time_pooling         None
06/08 10:48:08   train                True
06/08 10:48:08   train_initial_states True
06/08 10:48:08   train_prefix         'train'
06/08 10:48:08   truncate_lines       True
06/08 10:48:08   update_first         False
06/08 10:48:08   use_baseline         False
06/08 10:48:08   use_dropout          True
06/08 10:48:08   use_lstm             True
06/08 10:48:08   use_lstm_full_state  False
06/08 10:48:08   use_previous_word    True
06/08 10:48:08   verbose              True
06/08 10:48:08   vocab_prefix         'vocab'
06/08 10:48:08   weight_scale         0.1
06/08 10:48:08   word_dropout         0.0
06/08 10:48:08 python random seed: 7964202106146604465
06/08 10:48:08 tf random seed:     7203623748117120531
06/08 10:48:08 creating model
06/08 10:48:08 using device: /gpu:0
06/08 10:48:08 copying vocab to ../SLTU_experiments/grapheme/exp2/model/data/vocab.it
06/08 10:48:08 copying vocab to ../SLTU_experiments/grapheme/exp2/model/data/vocab.gr
06/08 10:48:08 reading vocabularies
06/08 10:48:08 creating model
06/08 10:48:18 model parameters (27)
06/08 10:48:18   baseline_step:0 ()
06/08 10:48:18   decoder_gr/attention_it/U_a/kernel:0 (24, 12)
06/08 10:48:18   decoder_gr/attention_it/W_a/bias:0 (12,)
06/08 10:48:18   decoder_gr/attention_it/W_a/kernel:0 (12, 12)
06/08 10:48:18   decoder_gr/attention_it/v_a:0 (12,)
06/08 10:48:18   decoder_gr/basic_lstm_cell/bias:0 (48,)
06/08 10:48:18   decoder_gr/basic_lstm_cell/kernel:0 (48, 48)
06/08 10:48:18   decoder_gr/it/initial_state_projection/bias:0 (24,)
06/08 10:48:18   decoder_gr/it/initial_state_projection/kernel:0 (12, 24)
06/08 10:48:18   decoder_gr/maxout/bias:0 (12,)
06/08 10:48:18   decoder_gr/maxout/kernel:0 (48, 12)
06/08 10:48:18   decoder_gr/softmax1/bias:0 (43,)
06/08 10:48:18   decoder_gr/softmax1/kernel:0 (6, 43)
06/08 10:48:18   embedding_gr:0 (43, 12)
06/08 10:48:18   embedding_it:0 (439, 12)
06/08 10:48:18   encoder_it/initial_state_bw:0 (24,)
06/08 10:48:18   encoder_it/initial_state_fw:0 (24,)
06/08 10:48:18   encoder_it/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/basic_lstm_cell/bias:0 (48,)
06/08 10:48:18   encoder_it/stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/basic_lstm_cell/kernel:0 (24, 48)
06/08 10:48:18   encoder_it/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/bias:0 (48,)
06/08 10:48:18   encoder_it/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/kernel:0 (24, 48)
06/08 10:48:18   global_step:0 ()
06/08 10:48:18   initial_state_keep_prob:0 ()
06/08 10:48:18   initial_state_keep_prob_1:0 ()
06/08 10:48:18   learning_rate:0 ()
06/08 10:48:18   rnn_input_keep_prob:0 ()
06/08 10:48:18   rnn_input_keep_prob_1:0 ()
06/08 10:48:18 number of parameters: 0.01M
06/08 10:48:18 global step: 0
06/08 10:48:19 baseline step: 0
06/08 10:48:19 reading training data
06/08 10:48:19 total line count: 297
06/08 10:48:19 files: ../SLTU_experiments/grapheme/files/train.it ../SLTU_experiments/grapheme/files/train.gr
06/08 10:48:19 lines reads: 297
06/08 10:48:19 reading development data
06/08 10:48:19 files: ../SLTU_experiments/grapheme/files/dev.it ../SLTU_experiments/grapheme/files/dev.gr
06/08 10:48:19 lines reads: 33
06/08 10:48:19 files: ../SLTU_experiments/grapheme/files/train.it ../SLTU_experiments/grapheme/files/train.gr
06/08 10:48:19 lines reads: 297
06/08 10:48:19 starting training
06/08 11:12:57 step 10000 epoch 1078 learning rate 0.001 step-time 0.146 loss 81.724
06/08 11:12:57 starting evaluation
06/08 11:12:59 dev bleu=13.62 loss=74.43 penalty=0.886 ratio=0.892
06/08 11:13:03 train bleu=11.20 loss=59.02 penalty=1.000 ratio=1.229
06/08 11:13:03 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 11:13:03 finished saving model
06/08 11:13:03 new best model
06/08 11:39:16 step 20000 epoch 2155 learning rate 0.001 step-time 0.156 loss 70.472
06/08 11:39:16 starting evaluation
06/08 11:39:17 dev bleu=15.53 loss=74.10 penalty=0.910 ratio=0.914
06/08 11:39:21 train bleu=16.03 loss=52.65 penalty=1.000 ratio=1.132
06/08 11:39:21 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 11:39:21 finished saving model
06/08 11:39:21 new best model
06/08 12:07:28 step 30000 epoch 3233 learning rate 0.001 step-time 0.167 loss 66.869
06/08 12:07:28 starting evaluation
06/08 12:07:29 dev bleu=16.15 loss=73.11 penalty=1.000 ratio=1.020
06/08 12:07:33 train bleu=19.20 loss=49.59 penalty=1.000 ratio=1.104
06/08 12:07:33 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 12:07:33 finished saving model
06/08 12:07:33 new best model
06/08 12:36:46 step 40000 epoch 4310 learning rate 0.001 step-time 0.173 loss 64.741
06/08 12:36:46 starting evaluation
06/08 12:36:47 dev bleu=14.99 loss=74.60 penalty=0.928 ratio=0.931
06/08 12:36:52 train bleu=19.40 loss=47.99 penalty=1.000 ratio=1.132
06/08 12:36:52 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 12:36:52 finished saving model
06/08 13:06:14 step 50000 epoch 5388 learning rate 0.001 step-time 0.174 loss 63.358
06/08 13:06:14 starting evaluation
06/08 13:06:14 dev bleu=16.99 loss=74.80 penalty=0.966 ratio=0.967
06/08 13:06:19 train bleu=19.50 loss=46.92 penalty=1.000 ratio=1.149
06/08 13:06:19 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 13:06:19 finished saving model
06/08 13:06:19 new best model
06/08 13:35:48 step 60000 epoch 6465 learning rate 0.001 step-time 0.175 loss 62.492
06/08 13:35:48 starting evaluation
06/08 13:35:48 dev bleu=16.24 loss=75.43 penalty=1.000 ratio=1.000
06/08 13:35:53 train bleu=20.77 loss=45.96 penalty=1.000 ratio=1.110
06/08 13:35:53 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 13:35:53 finished saving model
06/08 14:05:16 step 70000 epoch 7543 learning rate 0.001 step-time 0.174 loss 61.688
06/08 14:05:16 starting evaluation
06/08 14:05:17 dev bleu=16.12 loss=76.77 penalty=0.879 ratio=0.886
06/08 14:05:21 train bleu=21.66 loss=45.84 penalty=1.000 ratio=1.051
06/08 14:05:21 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 14:05:21 finished saving model
06/08 14:34:43 step 80000 epoch 8620 learning rate 0.001 step-time 0.174 loss 61.153
06/08 14:34:43 starting evaluation
06/08 14:34:43 dev bleu=16.04 loss=76.01 penalty=0.929 ratio=0.932
06/08 14:34:48 train bleu=19.93 loss=45.23 penalty=1.000 ratio=1.110
06/08 14:34:48 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 14:34:48 finished saving model
06/08 15:04:05 step 90000 epoch 9697 learning rate 0.001 step-time 0.174 loss 60.668
06/08 15:04:05 starting evaluation
06/08 15:04:05 dev bleu=17.07 loss=76.24 penalty=0.942 ratio=0.943
06/08 15:04:10 train bleu=20.41 loss=44.69 penalty=1.000 ratio=1.096
06/08 15:04:10 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 15:04:10 finished saving model
06/08 15:04:10 new best model
06/08 15:33:28 step 100000 epoch 10775 learning rate 0.001 step-time 0.174 loss 60.164
06/08 15:33:28 starting evaluation
06/08 15:33:28 dev bleu=17.44 loss=76.60 penalty=0.988 ratio=0.988
06/08 15:33:33 train bleu=22.36 loss=44.42 penalty=1.000 ratio=1.029
06/08 15:33:33 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 15:33:33 finished saving model
06/08 15:33:33 new best model
06/08 16:02:43 step 110000 epoch 11852 learning rate 0.001 step-time 0.173 loss 59.697
06/08 16:02:43 starting evaluation
06/08 16:02:43 dev bleu=14.51 loss=76.65 penalty=1.000 ratio=1.105
06/08 16:02:47 train bleu=23.00 loss=44.21 penalty=1.000 ratio=1.046
06/08 16:02:47 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 16:02:47 finished saving model
06/08 16:31:58 step 120000 epoch 12930 learning rate 0.001 step-time 0.173 loss 59.211
06/08 16:31:58 starting evaluation
06/08 16:31:58 dev bleu=15.70 loss=77.52 penalty=1.000 ratio=1.067
06/08 16:32:03 train bleu=24.10 loss=43.85 penalty=1.000 ratio=1.006
06/08 16:32:03 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 16:32:03 finished saving model
06/08 17:01:11 step 130000 epoch 14007 learning rate 0.001 step-time 0.173 loss 58.812
06/08 17:01:11 starting evaluation
06/08 17:01:12 dev bleu=14.34 loss=77.92 penalty=0.982 ratio=0.982
06/08 17:01:16 train bleu=22.94 loss=43.76 penalty=1.000 ratio=1.053
06/08 17:01:16 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 17:01:16 finished saving model
06/08 17:30:23 step 140000 epoch 15085 learning rate 0.001 step-time 0.173 loss 58.428
06/08 17:30:23 starting evaluation
06/08 17:30:23 dev bleu=14.62 loss=78.03 penalty=1.000 ratio=1.008
06/08 17:30:27 train bleu=24.04 loss=43.46 penalty=1.000 ratio=1.011
06/08 17:30:27 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 17:30:27 finished saving model
06/08 17:59:32 step 150000 epoch 16162 learning rate 0.001 step-time 0.173 loss 58.143
06/08 17:59:32 starting evaluation
06/08 17:59:32 dev bleu=14.08 loss=78.63 penalty=1.000 ratio=1.004
06/08 17:59:37 train bleu=23.41 loss=43.45 penalty=1.000 ratio=1.036
06/08 17:59:37 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 17:59:37 finished saving model
06/08 17:59:37 finished training
06/08 17:59:37 exiting...
06/08 17:59:37 saving model to ../SLTU_experiments/grapheme/exp2/model/checkpoints
06/08 17:59:37 finished saving model
